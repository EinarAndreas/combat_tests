Reproduction and alternative analysis of "Data Set 2" from "Adjusting batch effects in microarray data using Empirical Bayes methods."
========================================================

Overview
-----

This report aims to show to what extent the use of ComBat led to false results in the second "proof of concept" analysis performed in Johnson et al. The example named "Data Set 2" and their analysis are described in the supplementary materials for Johnson et al.
Description on how to obtain the data were found [here](https://groups.google.com/forum/#!msg/combat-user-forum/S9vBcXw8RGk/QIoD6oRBM9IJ) which had links to [dataExample2.txt](http://www.bu.edu/jlab/wp-assets/ComBat/data/dataExample2.txt)
and [sampleInfoExample2.txt](http://www.bu.edu/jlab/wp-assets/ComBat/data/sampleInfoExample2.txt)

This document has four main parts
- Reproduce some of the results to show that we are working on the same data and analysis workflow.
- Remove the use of ComBat and perform the same analysis with alternative established tools
- Estimate the error introduced by ComBat and its consequences for the conclusion of the study
- Perform a few more sanity checks to substantiate that the difference in results for the two above analysis is an error introduced by ComBat.

Read data and sample annotation
-------------------------------------------------------

The data files consist of 35 samples of which 5 are annotated as "WT" and were not
refered to in Johnson et al. These are taken out in the beginning and not used in this report.


```{r, results='hide'}
library(pheatmap)
library(RColorBrewer)
library(sva)
library(qvalue)
library(limma)
```
```{r, cache=TRUE}

orgdatamatrix = as.matrix(read.table("data/dataExample2.txt", sep="\t", header=TRUE))
datamatrix=orgdatamatrix[1:3000,]# for dev.
#datamatrix=orgdatamatrix# for dev.
sampleannotation = read.table("data/sampleInfoExample2.txt", 
                              sep="\t", header=TRUE, stringsAsFactors=FALSE)
rownames(sampleannotation)=sampleannotation$ArrayName
sampleannotation$Batch=factor(as.character(sampleannotation$Batch)) 
sampleannotation$Cell=factor(as.character(sampleannotation$Cell)) 
# must be discrete for the pheatmap
#table(sampleannotation$ArrayName==dimnames(datamatrix)[[2]]) # ordercheck
datamatrix=datamatrix[,sampleannotation$Type!="WT"]
sampleannotation=sampleannotation[sampleannotation$Type!="WT",]
sampleannotation$Type=factor(sampleannotation$Type)
print(dim(datamatrix))
#dev/debug
useparprior=TRUE
```
The covariate/batch balance;
```{r}
print(table(sampleannotation[ , c("Batch", "Type")])  )
```

Reproduce results
-------------------------------------------------------
Following the description in section A.1 and A.2 and figure texts in the supplementary materials for Johnson et al. we try to reproduce some of their results, first the heatmap in Figure A.1.
> A heatmap clustering of data set 2. 698 genes with large variation across all the samples are clustered.<cite> Johnson et al.

```{r, tidy=FALSE}
variatonmeasure = apply(datamatrix, 1, FUN= function(x){var(x)})
clustermatrix = datamatrix[order(variatonmeasure, decreasing=TRUE),][1:2698,]
Batchcol = brewer.pal(8,"Set2")[1:3]
names(Batchcol) = levels(sampleannotation$Batch)
Typecol =brewer.pal(8,"Set2")[4:5]
names(Typecol) = levels(sampleannotation$Type)
ann_colors = list(Batch = Batchcol, Type = Typecol)
```
```{r reproduced_figA1, dev='png', fig.width=8, fig.height=15, fig.cap="fig. 1"}
pheatmap(clustermatrix, scale = "row", cluster_rows=T, cluster_cols=T, 
         color = colorRampPalette(c("red", "black", "green"))(n = 299),
         annotation = sampleannotation[, c("Batch", "Type")],  annotation_colors = ann_colors,
         cellheight=NA, cellwidth=13,fontsize=12,  drop_levels=TRUE, show_rownames=F,
         main=paste("Reproduced Fig A1.",sep=""),border_color=NA,treeheight_row=0)

```
The Heatmap does not look exactly as Fig. A1 in the paper. This could be due to undocumented transformations preformed in Johnson et al., for example log-transformation or standarization. And there are different ways of calculating variation for a probe. What parameters that were used for the clustring could also be different. But the purpose of the figure was to show that a batch effect is present in the data and espeically for batch 3. This is also achived in the reproduced figure.

Next 3 datasets were made,
- EB2: batches 1 and 2 adjusted with nonparametric use of ComBat
- EB3: All thre batches adjusted with nonparametric use of ComBat
- Batch3: Only batch 3 (no adjustments)
```{r,results='hide', tidy=FALSE}
mat = datamatrix[,sampleannotation$Batch %in% c("1","2")]
EB2 = as.matrix( sva::ComBat(
            dat=mat, 
            batch=sampleannotation[colnames(mat),"Batch"], 
            mod=model.matrix( ~as.factor(sampleannotation[colnames(mat),"Type"])  ), 
            numCovs=NULL, 
            par.prior=useparprior, 
            prior.plots=FALSE))

mat = datamatrix[, ]
EB3 = as.matrix( sva::ComBat(
            dat=mat, 
            batch=sampleannotation[colnames(mat),"Batch"], 
            mod=model.matrix(~as.factor(sampleannotation[colnames(mat),"Type"])), 
            numCovs=NULL, 
            par.prior=useparprior, 
            prior.plots=FALSE))

Batch3 = datamatrix[,sampleannotation$Batch=="3"]
```

Next we try to re-create Figure A.2

> A heatmap diagram of 770 genes from data set 2 after applying the EB batch adjustments.<cite> Johnson et al.

There is no description in how the 770 genes were selected so we use the same approach as in the reprodution of Figure A.1.
```{r reproduced_figA2, dev='png', fig.width=8, fig.height=15}
clustermatrix = EB3
variatonmeasure = apply(clustermatrix, 1, FUN= function(x){var(x)})
clustermatrix = clustermatrix[order(variatonmeasure, decreasing=TRUE),][1:770,]
Batchcol = brewer.pal(8,"Set2")[1:3]
names(Batchcol) = levels(sampleannotation$Batch)
Typecol =brewer.pal(8,"Set2")[4:6]
names(Typecol) = levels(sampleannotation$Type)
Cellcol =brewer.pal(8,"Set1")[1:5]
names(Cellcol) = levels(sampleannotation$Cell)
ann_colors = list(Batch = Batchcol, Type = Typecol, Cell=Cellcol)
pheatmap(clustermatrix, scale = "row", cluster_rows=T, cluster_cols=T, 
         color = colorRampPalette(c("red", "black", "green"))(n = 299),
         annotation = sampleannotation[, c("Batch", "Type", "Cell")],  
         cellheight=NA, cellwidth=13,fontsize=12,  drop_levels=TRUE, show_rownames=F,
         main=paste("Reproduced Fig A2.",sep=""),border_color=NA,treeheight_row=0)

```
Again the heatmap is not exactly as in Johnson et al, but the batch clustring is broken and the cell clones with the same number seem to cluster.

Now follows a string of differentially expression test for different data sets.
> Differential expression was assessed using Welch’s t-test to determine the differential expression of RNAi versus control samples. EB2 produced at list of 86 significant genes at a false discovery (q-value) threshold of 0.05 (Storey and Tibshirani, 2003).<cite> Johnson et al.

```{r, tidy=FALSE}
EB2_pvals = apply(EB2, 1 , 
                  FUN=function(x){t.test(
                                    x[sampleannotation[colnames(EB2), "Type"]=="C"],
                                    x[sampleannotation[colnames(EB2), "Type"]=="R"]
                                    )$p.value})
print(table(qvalue(EB2_pvals)$qvalue<0.05))
```



> The third batch alone produced a list of 37 significant genes using the same threshold. Crossing the significant gene lists, we observed 13 genes common in both lists (Fisher’s exact p-value < 1e-15). <cite> Johnson et al.

```{r, tidy=FALSE}
Batch3_pvals = apply(Batch3, 1 , 
                  FUN=function(x){t.test(
                                    x[sampleannotation[colnames(Batch3), "Type"]=="C"],
                                    x[sampleannotation[colnames(Batch3), "Type"]=="R"]
                                    )$p.value})
print(table(qvalue(Batch3_pvals)$qvalue<0.05))
```



> Without any adjustment, combining these two batches produced a list of only 9 genes a q-value cutoff of 0.05<cite> Johnson et al.

```{r, tidy=FALSE}
Batch12 = datamatrix[,sampleannotation$Batch %in% c("1","2")]
Batch12_pvals = apply(Batch12, 1 , 
                  FUN=function(x){t.test(
                                    x[sampleannotation[colnames(Batch12), "Type"]=="C"],
                                    x[sampleannotation[colnames(Batch12), "Type"]=="R"]
                                    )$p.value})
print(table(qvalue(Batch12_pvals)$qvalue<0.05))
```


> Welch’s t-test was also applied to EB3 to find differential expressed genes; yielding 1599 genes significant at a q-value cutoff of 0.05. <cite> Johnson et al.

```{r, tidy=FALSE}
EB3_pvals = apply(EB3, 1 , 
                  FUN=function(x){t.test(
                                    x[sampleannotation[colnames(EB3), "Type"]=="C"],
                                    x[sampleannotation[colnames(EB3), "Type"]=="R"]
                                    )$p.value})
print(table(qvalue(EB3_pvals)$qvalue<0.05))
```

> Reducing the q-value threshold to 0.01 yielded 488 significant genes. <cite> Johnson et al.

```{r}
print(table(qvalue(EB3_pvals)$qvalue<0.01))
```

> ..decreasing the threshold further to 0.001 yielded 161 significant genes.<cite> Johnson et al.

```{r}
print(table(qvalue(EB3_pvals)$qvalue<0.001))
```

The reproduced number of significant probes are not the same as the reported ones, but they are not completly off. Again this is likly to be due to undocumented steps performed by Johnson et al.


Analysis without ComBat
--------------

An alternative (and better?) way of handling batch effect is to include it in the model for the statistical test. This is possible in several tools and we choose to use the popular limma package (Smyth et al). 

We start with the data before ComBat adjustment. Limma works best with log transformed and between array normalized values, so first we have to do something with the negative signals.  Then the test is run with the batch included as a blocking factor.
```{r}
thismat = datamatrix
thismat[thismat<1]=1
thismat = log2(thismat)
thismat=normalizeBetweenArrays(thismat)
Type = as.factor(sampleannotation$Type)
Block = as.factor(sampleannotation$Batch)
design <- model.matrix(~0+Type+Block)
fit <- lmFit(thismat, design)
cont.matrix = makeContrasts ( contrasts="TypeR-TypeC", levels=design)	
fit2 = contrasts.fit(fit, cont.matrix)
datamatrix_limma_res <- eBayes(fit2)
print(table(qvalue(datamatrix_limma_res$p.value[,1])$qvalue<0.05))
```

Consequences of ComBat use for the end result
--------------

Since limma was not the tool used in Johnson et al.s workflow, we will first create the corresponding analysis in Limma for the whole ComBat adjusted data set in order to make a fair comparison.
```{r}
thismat = EB3
thismat[thismat<1]=1
thismat = log2(thismat)
thismat=normalizeBetweenArrays(thismat)
Type = as.factor(sampleannotation$Type)
design = model.matrix(~0 + Type)
fit <- lmFit(thismat, design)
cont.matrix = makeContrasts ( contrasts="TypeR-TypeC", levels=design)  
fit2 = contrasts.fit(fit, cont.matrix)
EB3_limma_res <- eBayes(fit2)
print(table(qvalue(EB3_limma_res$p.value[,1])$qvalue<0.05))
```
The number of genes below the significance threshold is fairly similar to the t-test.

Now we can compare the P-values for the two alternative procedures.
```{r, dev='svg', fig.width=8, fig.height=8}
hist(EB3_limma_res$p.value[,1], border="blue", main="P-values, ComBat vs Limma", breaks=100, xlab="p-value")
hist(datamatrix_limma_res$p.value[,1], border="red", add=T, breaks=100)
legend("topright", legend=c("ComBat adjusted", "Limma adjusted"), text.col=c("blue", "red"))
```
This plot shows that the p-values calculated from the ComBat adjusted data is skewed towards the low end compared to when batch is considered inside the statistical test in limma. However, the difference is not large and from this plot alone it could be argued that ComBat is just better.

Lets assume one aim of this study was to obtain a list of genes that differed between control and treatment with a significant cut-off of q<0.05. Then handling the batch effect with ComBat adjustment will yield a list of `r sum(qvalue(EB3_limma_res$p.value[,1])$qvalue<0.05)` wheras a list of `r sum(qvalue(datamatrix_limma_res$p.value[,1])$qvalue<0.05)` probes would have been closer to the truth. Also it is of interest to note that it is mostly the same probes that are found.
```{r, tidy=FALSE}
table( qvalue(datamatrix_limma_res$p.value[,1])$qvalue<0.05 ,
         qvalue(EB3_limma_res$p.value[,1])$qvalue<0.05,
       dnn=c("limma", "ComBat"))
```
And if they were to use the top 1000 probes in a gene set test they found have used many of the same genes regardless of the use of ComBat.
```{r, tidy=FALSE}
table( rank(datamatrix_limma_res$p.value[,1])  <=1000 & 
         rank(EB3_limma_res$p.value[,1])<=1000   )
```
Our conclusion is that for this study the error introduced by the use of ComBat would only have a modest effect on the final result.

Additional sanity checks
---------
For this data set ComBat's handling of batch-effects yields only modestly different results compared to the alternative. Explanation for the difference could be that ComBat is better. To substantiate that the result from the use of ComBat is less trustworthy we provide a few additional sanity checks.

First we use random numbers drawn from the same distribution regardless of batch or covariate but retain the batch/covariate design.
```{r, results='hide', tidy=FALSE}
set.seed(100)
randommatrix = datamatrix
randommatrix[,] =rnorm(length(datamatrix), mean=0, sd=1)
EBrand = as.matrix( sva::ComBat(
            dat=randommatrix, 
            batch=sampleannotation[colnames(randommatrix),"Batch"], 
            mod=model.matrix(~as.factor(sampleannotation[colnames(randommatrix),"Type"])), 
            numCovs=NULL, 
            par.prior=useparprior, 
            prior.plots=FALSE))
```
Limma is then used in 3 ways
- On the ComBat adjusted random numbers
- On the random numbers including batch as a blocking factor
- On the random numbers ignorant to batch information
```{r, tidy=FALSE}

Type = as.factor(sampleannotation$Type)
design = model.matrix(~0 + Type)
cont.matrix = makeContrasts ( contrasts="TypeR-TypeC", levels=design)  

fit <- lmFit(EBrand, design)
fit2 = contrasts.fit(fit, cont.matrix)
fit2 <- eBayes(fit2)
EBrand_limma_pvalues = fit2$p.value[,1]

fit <- lmFit(randommatrix, design)
fit2 = contrasts.fit(fit, cont.matrix)
fit2 <- eBayes(fit2)
randommatrix_limma_pvalues = fit2$p.value[,1]


Block = as.factor(sampleannotation$Batch)
design <- model.matrix(~0+Type+Block)
fit <- lmFit(randommatrix, design)
cont.matrix = makeContrasts ( contrasts="TypeR-TypeC", levels=design) 
fit2 = contrasts.fit(fit, cont.matrix)
fit2 <- eBayes(fit2)
randommatrix_limma_batch_pvalues = fit2$p.value[,1]


```
Plotting the p-values
```{r, dev='svg', fig.width=8, fig.height=8}
hist(EBrand_limma_pvalues, border="blue",  main="P-values, Random numbers", breaks=100, xlab="p-value")
hist(randommatrix_limma_pvalues, border="black", add=T, breaks=100)
hist(randommatrix_limma_batch_pvalues, border="red", add=T, breaks=100)
legend("topright", legend=c("ComBat adjusted", "Limma adjusted", "No adjustment"), text.col=c("blue", "black", "red"))
```
The p-values for ComBat adjusted random numbers are enriched slighlty for low p-values. Indicating that the same enrichment seen for the real data is also false.

Another sanity check is to subset the real data into fictive batches. The EB2 data set contains the same number of samples with the same "Type" balance as the third batch alone. 
>The third batch was used for comparison against the EB2 analysis results because it was an identical experiment to EB2 other than the fact that it was conducted in a single batch.
Thus it is interesting to inspect the p-value from these two indentical experiments, EB2 (batch 1 and 2 adjusted with ComBat) and batch 3 (no ComBat adjustment). These p-values are computed above.
```{r, dev='svg', fig.width=8, fig.height=8}
hist(EB2_pvals, border="blue",  main="P-values, real data", breaks=100, xlab="p-value")
hist(Batch3_pvals, border="red", add=T, breaks=100)
legend("topright", legend=c("ComBat adjusted batch 1 and 2", "Batch 3"), text.col=c("blue", "red"))
```
The 2-batch experiment is able to retrive more significant genes than running all in one batch. A further test of this is to split batch 3 into two imaginary batches in the same design as for batch 1 and 2, and then look at the p-value distribution compared with the real batch 3.

```{r, results='hide', tidy=FALSE}
Batch45 = Batch3
Batch45_annot = sampleannotation[dimnames(Batch45)[[2]],]
Batch45_annot$Batch="4"
Batch45_annot$Batch[Batch45_annot$Type=="C"][1:4] = "5"
Batch45_annot$Batch[Batch45_annot$Type=="R"][1:3] = "5"

table(sampleannotation[sampleannotation$Batch %in% c("1", "2"), c("Batch", "Type")])
table(Batch45_annot[, c("Batch", "Type")])

EB45 = as.matrix( sva::ComBat(
            dat=Batch45, 
            batch=Batch45_annot[colnames(Batch45),"Batch"], 
            mod=model.matrix( ~as.factor(Batch45_annot[colnames(Batch45),"Type"])  ), 
            numCovs=NULL, 
            par.prior=useparprior, 
            prior.plots=FALSE))

EB45_pvals = apply(EB45, 1 , 
                  FUN=function(x){t.test(
                                    x[Batch45_annot[colnames(EB45), "Type"]=="C"],
                                    x[Batch45_annot[colnames(EB45), "Type"]=="R"]
                                    )$p.value})
```
```{r, dev='svg', fig.width=8, fig.height=8, tidy=FALSE}
hist(EB45_pvals, border="blue",  main="P-values, real data", breaks=100, xlab="p-value")
hist(Batch3_pvals, border="red", add=T, breaks=100)
legend("topright", legend=c("ComBat adjusted pseudo-batches of batch 3", "Batch 3 not adjusted"), text.col=c("blue", "red"))
```


Done!



