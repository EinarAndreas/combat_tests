

Reproduction and alternative analysis of "Data Set 2" from "Adjusting batch effects in microarray data using Empirical Bayes methods."
========================================================

```{r, results='hide', echo=FALSE}
starttime = Sys.time()
```
`r as.character(starttime)`

### Overview
This report aims to show to what extent the use of ComBat led to false results in the second analysis example given in [Johnson et al.](http://biostatistics.oxfordjournals.org/content/8/1/118.abstract) The example named "Data Set 2" and the analysis are described in the [supplementary material](http://biostatistics.oxfordjournals.org/content/suppl/2006/04/21/kxj037.DC1/kxj037supp.pdf) for Johnson et al.
Description on how to obtain the data were found [here](https://groups.google.com/forum/#!msg/combat-user-forum/S9vBcXw8RGk/QIoD6oRBM9IJ) which had links to [dataExample2.txt](http://www.bu.edu/jlab/wp-assets/ComBat/data/dataExample2.txt)
and [sampleInfoExample2.txt](http://www.bu.edu/jlab/wp-assets/ComBat/data/sampleInfoExample2.txt)

This document has four main parts
- Reproduce some of the results to show that we are working on the same data and analysis workflow
- Remove the use of ComBat and perform the same analysis with an alternative established tool
- Estimate the error introduced by ComBat and the consequences for the conclusion of the study
- Perform a few more sanity checks to substantiate that the difference in results for the two above analysis is mainly due to error introduced by ComBat  

        
### Read data and sample annotation
The data files consist of 35 samples of which 5 are annotated as "WT" and were not
referred to in Johnson et al. These are taken out in the beginning and are not used in this report.

```{r, results='hide',  message=FALSE, error=FALSE}
includelibs = c("pheatmap", "sva", "qvalue", "limma")
tmp = lapply(includelibs, require, character.only=T)
print(tmp)
if(any(!unlist(tmp)))
{
  stop( paste("Not able to find all packages. Please install ",
              paste(includelibs[!unlist(tmp)], collapse=", ") )
              )
}
rm(tmp)
```
```{r, cache=TRUE, tidy=FALSE}
datamatrix = as.matrix(read.table("data/dataExample2.txt", sep="\t", header=TRUE))#[1:3000,]#dev.
sampleannotation = read.table("data/sampleInfoExample2.txt", 
                              sep="\t", header=TRUE, stringsAsFactors=FALSE)
rownames(sampleannotation)=sampleannotation$ArrayName
sampleannotation$Batch=factor(as.character(sampleannotation$Batch)) 
sampleannotation$Cell=factor(as.character(sampleannotation$Cell)) 
# must be discrete for the pheatmap
#table(sampleannotation$ArrayName==dimnames(datamatrix)[[2]])#ordercheck
datamatrix=datamatrix[,sampleannotation$Type!="WT"]
sampleannotation=sampleannotation[sampleannotation$Type!="WT",]
sampleannotation$Type=factor(sampleannotation$Type)
#dev/debug
useparprior=TRUE
print(dim(datamatrix))

```
Inspection of the covariate/batch balance;
```{r}
print(table(sampleannotation[ , c("Batch", "Type")])  )
```

### Reproduce the original results
Following the description in section A.1 and A.2 and figure texts in the supplementary materials for Johnson et al., we try to reproduce some of their results. First, the heatmap in Figure A.1.
> A heatmap clustering of data set 2. 698 genes with large variation across all the samples are clustered.<cite> Johnson et al.

```{r reproduced_figA1, dev='png', fig.width=8, fig.height=15, fig.cap="fig. 1", tidy=FALSE, results='hold', fig.show='hold'}
variationmeasure = apply(datamatrix, 1, FUN= function(x){var(x)})
clustermatrix = datamatrix[order(variationmeasure, decreasing=TRUE),][1:2698,]
pheatmap(clustermatrix, scale = "row", cluster_rows=T, cluster_cols=T, 
         color = colorRampPalette(c("red", "black", "green"))(n = 299),
         annotation = sampleannotation[, c("Batch", "Type")], 
         cellheight=NA, cellwidth=13,fontsize=12,  drop_levels=TRUE, show_rownames=F,
         main=paste("Reproduced Fig A1.",sep=""),border_color=NA,treeheight_row=0)
rm(variationmeasure, clustermatrix)
```
The heatmap does not look exactly as Fig. A1 in the paper. This could be due to undocumented transformations preformed in Johnson et al., for example log-transformation or standardization. And there are different ways of calculating variation for a probe. What parameters that were used for the clustering could also be different. But the purpose of the figure was to show that a batch effect is present in the data and especially for batch 3. This is also achieved in the reproduced figure.

Next 3 data sets were made,
- **EB2**: Batches 1 and 2 adjusted with non-parametric use of ComBat
- **EB3**: All thee batches adjusted with non-parametric use of ComBat
- **Batch3**: Only batch 3 (no adjustments)
```{r,results='hide', tidy=FALSE}
mat = datamatrix[,sampleannotation$Batch %in% c("1","2")]
EB2 = as.matrix( sva::ComBat(
            dat=mat, 
            batch=sampleannotation[colnames(mat),"Batch"], 
            mod=model.matrix( ~as.factor(sampleannotation[colnames(mat),"Type"])  ), 
            numCovs=NULL, 
            par.prior=useparprior, 
            prior.plots=FALSE))
rm(mat)
EB3 = as.matrix( sva::ComBat(
            dat=datamatrix, 
            batch=sampleannotation[colnames(datamatrix),"Batch"], 
            mod=model.matrix(~as.factor(sampleannotation[colnames(datamatrix),"Type"])), 
            numCovs=NULL, 
            par.prior=useparprior, 
            prior.plots=FALSE))
Batch3 = datamatrix[,sampleannotation$Batch=="3"]

```

Next we try to re-create Figure A.2

> A heatmap diagram of 770 genes from data set 2 after applying the EB batch adjustments.<cite> Johnson et al.

There is no description in how the 770 genes were selected so we use the same approach as in the reproduction of Figure A.1.
```{r reproduced_figA2, dev='png', fig.width=8, fig.height=15, fig.show='hold', tidy=FALSE}
variationmeasure = apply(EB3, 1, FUN= function(x){var(x)})
clustermatrix = EB3[order(variationmeasure, decreasing=TRUE),][1:770,]
pheatmap(clustermatrix, scale = "row", cluster_rows=T, cluster_cols=T, 
         color = colorRampPalette(c("red", "black", "green"))(n = 299),
         annotation = sampleannotation[, c("Batch", "Type", "Cell")],  
         cellheight=NA, cellwidth=13,fontsize=12,  drop_levels=TRUE, show_rownames=F,
         main=paste("Reproduced Fig A2.",sep=""),border_color=NA,treeheight_row=0)
rm(variationmeasure, clustermatrix)
```
Again the heatmap is not exactly as in Johnson et al, but the batch clustering is broken("Batch, bottom annotation row") and the samples cluster more by cell type("Cell", top annotation row) and treatment type("Type", middle annotation row).

Now follows a few tests for differentially expressed probes.
> Differential expression was assessed using Welch’s t-test to determine the differential expression of RNAi versus control samples. EB2 produced at list of 86 significant genes at a false discovery (q-value) threshold of 0.05 (Storey and Tibshirani, 2003).<cite> Johnson et al.

```{r, tidy=FALSE}
EB2_pvals = apply(EB2, 1 , 
                  FUN=function(x){t.test(
                                    x[sampleannotation[colnames(EB2), "Type"]=="C"],
                                    x[sampleannotation[colnames(EB2), "Type"]=="R"]
                                    )$p.value})
print(table(qvalue(EB2_pvals)$qvalue<0.05))
```
Original number was **86**, reproduced number is **`r sum(qvalue(EB2_pvals)$qvalue<0.05)`**.


> The third batch alone produced a list of 37 significant genes using the same threshold. <cite> Johnson et al.

```{r, tidy=FALSE}
Batch3_pvals = apply(Batch3, 1 , 
                  FUN=function(x){t.test(
                                    x[sampleannotation[colnames(Batch3), "Type"]=="C"],
                                    x[sampleannotation[colnames(Batch3), "Type"]=="R"]
                                    )$p.value})
print(table(qvalue(Batch3_pvals)$qvalue<0.05))
```
Original number was **37**, reproduced number is **`r sum(qvalue(Batch3_pvals)$qvalue<0.05)`**.


> Without any adjustment, combining these two batches produced a list of only 9 genes a q-value cutoff of 0.05<cite> Johnson et al.

```{r, tidy=FALSE}
Batch12 = datamatrix[,sampleannotation$Batch %in% c("1","2")]
Batch12_pvals = apply(Batch12, 1 , 
                  FUN=function(x){t.test(
                                    x[sampleannotation[colnames(Batch12), "Type"]=="C"],
                                    x[sampleannotation[colnames(Batch12), "Type"]=="R"]
                                    )$p.value})
print(table(qvalue(Batch12_pvals)$qvalue<0.05))
```
Original number was **9**, reproduced number is **`r sum(qvalue(Batch12_pvals)$qvalue<0.05)`**.

> Welch’s t-test was also applied to EB3 to find differential expressed genes; yielding 1599 genes significant at a q-value cutoff of 0.05. <cite> Johnson et al.

```{r, tidy=FALSE}
EB3_pvals = apply(EB3, 1 , 
                  FUN=function(x){t.test(
                                    x[sampleannotation[colnames(EB3), "Type"]=="C"],
                                    x[sampleannotation[colnames(EB3), "Type"]=="R"]
                                    )$p.value})
print(table(qvalue(EB3_pvals)$qvalue<0.05))
```
Original number was **1599**, reproduced number is **`r sum(qvalue(EB3_pvals)$qvalue<0.05)`**.

> Reducing the q-value threshold to 0.01 yielded 488 significant genes. <cite> Johnson et al.

```{r}
print(table(qvalue(EB3_pvals)$qvalue<0.01))
```
Original number was **488**, reproduced number is **`r sum(qvalue(EB3_pvals)$qvalue<0.01)`**.

> ..decreasing the threshold further to 0.001 yielded 161 significant genes.<cite> Johnson et al.

```{r}
print(table(qvalue(EB3_pvals)$qvalue<0.001))
```
Original number was **161**, reproduced number is **`r sum(qvalue(EB3_pvals)$qvalue<0.001)`**.

The reproduced numbers are not the same as the reported ones, but they are not completely off. Again this is likely to be due to undocumented steps performed by Johnson et al.



### Analysis without ComBat
An alternative (and better?) way of handling batch effect is to include it in the model for the statistical test. This is possible in several tools and we choose to use the popular limma package (Smyth et al). 

We start with the data before ComBat adjustment. Limma works best with log transformed and between array normalized values, so first we set negative values to 1 and filter out probes that has this in more than half the samples. To ease later comparisons the same filter will also be applied to the EB3 data set. Then the test is run with the batch included as a blocking factor.
```{r}
negativeprobesfilter =( rowSums(datamatrix>=1) >= (0.5*ncol(datamatrix)) ) & 
                      ( rowSums(EB3>=1) >= (0.5*ncol(EB3)) )
thismat = datamatrix[negativeprobesfilter, ]
thismat[thismat<1]=1
thismat=normalizeBetweenArrays(log2(thismat))
Type = as.factor(sampleannotation$Type)
Block = as.factor(sampleannotation$Batch)
design = model.matrix(~0+Type+Block)
fit = lmFit(thismat, design)
cont.matrix = makeContrasts ( contrasts="TypeR-TypeC", levels=design)	
fit2 = contrasts.fit(fit, cont.matrix)
datamatrix_limma_res = eBayes(fit2)
rm(thismat, Type, Block, design, fit, cont.matrix, fit2)
print(table(qvalue(datamatrix_limma_res$p.value[,1])$qvalue<0.05))
```
Number of differentially expressed probes found with the alternative anaysis is **`r sum(qvalue(datamatrix_limma_res$p.value[,1])$qvalue<0.05)`**.

### Consequences of ComBat use for the end result

Since limma was not the tool used in Johnson et al.'s workflow, we will first need to create the corresponding analysis in Limma for the whole ComBat adjusted data set in order to make a fair comparison.
```{r}
thismat = EB3[negativeprobesfilter, ]
thismat[thismat<1]=1
thismat=normalizeBetweenArrays(log2(thismat))
Type = as.factor(sampleannotation$Type)
design = model.matrix(~0 + Type)
fit = lmFit(thismat, design)
cont.matrix = makeContrasts ( contrasts="TypeR-TypeC", levels=design)  
fit2 = contrasts.fit(fit, cont.matrix)
EB3_limma_res = eBayes(fit2)
rm(thismat, Type, design, fit, cont.matrix, fit2)
print(table(qvalue(EB3_limma_res$p.value[,1])$qvalue<0.05))

```
The number of genes below the significance threshold is fairly similar to the t-test.

Now we can compare the P-values for the two alternative procedures.
```{r, dev='svg', fig.width=8, fig.height=8}
hist(EB3_limma_res$p.value[,1], border="blue", main="P-values, ComBat vs Limma", breaks=100, xlab="p-value")
hist(datamatrix_limma_res$p.value[,1], border="red", add=T, breaks=100)
legend("topright", legend=c("ComBat adjusted", "Limma adjusted"), text.col=c("blue", "red"))
```
This plot shows that the p-values calculated from the ComBat adjusted data is skewed towards the low end compared to when batch is considered inside the statistical test in limma. However, the difference is not large and from this plot alone it could be argued that ComBat is just better.

Lets assume one aim of this study was to obtain a list of genes that differed between control and treatment with a significant cut-off of q<0.05. Then handling the batch effect with ComBat adjustment will yield a list of **`r sum(qvalue(EB3_limma_res$p.value[,1])$qvalue<0.05)`** whereas a list of **`r sum(qvalue(datamatrix_limma_res$p.value[,1])$qvalue<0.05)`** probes would have been closer to the truth. Also it is of interest to note that it is mostly the same probes that are found.
```{r, tidy=FALSE}
table( qvalue(datamatrix_limma_res$p.value[,1])$qvalue<0.05 ,
         qvalue(EB3_limma_res$p.value[,1])$qvalue<0.05,
       dnn=c("limma", "ComBat"))
```
And if they were to use the top 1000 probes in a gene set test, they would have found many of the same genes regardless of hadling batch effects by ComBat or limma.
```{r, tidy=FALSE}
table( rank(datamatrix_limma_res$p.value[,1])  <=1000 & 
         rank(EB3_limma_res$p.value[,1])<=1000   )
```
Our conclusion is that for this study the error introduced by the use of ComBat would probably have a modest effect on the final result.

### Additional sanity checks

To substantiate that the result from the use of ComBat is less trustworthy than the alternative analysis we provide a few additional sanity checks.

First we use random numbers drawn from the same distribution regardless of batch or covariate but retaining the batch/covariate design.
```{r, results='hide', tidy=FALSE}
set.seed(100)
randommatrix = datamatrix
randommatrix[,] =rnorm(length(datamatrix), mean=0, sd=1)
EBrand = as.matrix( sva::ComBat(
            dat=randommatrix, 
            batch=sampleannotation[colnames(randommatrix),"Batch"], 
            mod=model.matrix(~as.factor(sampleannotation[colnames(randommatrix),"Type"])), 
            numCovs=NULL, 
            par.prior=useparprior, 
            prior.plots=FALSE))
```
Limma is then used in 3 ways
- On the ComBat adjusted random numbers
- On the random numbers including batch as a blocking factor
- On the random numbers ignoring batch information
```{r, tidy=FALSE}

Type = as.factor(sampleannotation$Type)
design = model.matrix(~0 + Type)
cont.matrix = makeContrasts ( contrasts="TypeR-TypeC", levels=design)  

fit = lmFit(EBrand, design)
fit2 = contrasts.fit(fit, cont.matrix)
EBrand_limma_pvalues = eBayes(fit2)$p.value[,1]


fit = lmFit(randommatrix, design)
fit2 = contrasts.fit(fit, cont.matrix)
randommatrix_limma_pvalues = eBayes(fit2)$p.value[,1]



Block = as.factor(sampleannotation$Batch)
design = model.matrix(~0+Type+Block)
fit = lmFit(randommatrix, design)
cont.matrix = makeContrasts ( contrasts="TypeR-TypeC", levels=design) 
fit2 = contrasts.fit(fit, cont.matrix)
fit2 = eBayes(fit2)
randommatrix_limma_batch_pvalues = fit2$p.value[,1]


```
Plotting the p-values
```{r, dev='svg', fig.width=8, fig.height=8}
hist(EBrand_limma_pvalues, border="blue",  main="P-values, Random numbers", breaks=100, xlab="p-value")
hist(randommatrix_limma_pvalues, border="black", add=T, breaks=100)
hist(randommatrix_limma_batch_pvalues, border="red", add=T, breaks=100)
legend("topright", legend=c("ComBat adjusted", "Limma adjusted", "No adjustment"), text.col=c("blue", "black", "red"))
```
The p-values for ComBat adjusted random numbers are slightly enriched for low p-values. Indicating that the same enrichment seen in the real data is also false.

Another sanity check is to subset the real data into fictive batches. The idea behind this check stems from the observation that the EB2 data set contains the same number of samples with the same batch/covariate balance as the third batch alone. 
> The third batch was used for comparison against the EB2 analysis results because it was an identical experiment to EB2 other than the fact that it was conducted in a single batch. <cite> Johnson et al.

Thus it is interesting to inspect the p-value from these two identical experiments, EB2 (batch 1 and 2 adjusted with ComBat) and batch 3 (no ComBat adjustment). These p-values are already computed above.
```{r, dev='svg', fig.width=8, fig.height=8}
hist(EB2_pvals, border="blue",  main="P-values, real data", breaks=100, xlab="p-value")
hist(Batch3_pvals, border="red", add=T, breaks=100)
legend("topright", legend=c("ComBat adjusted batch 1 and 2", "Batch 3"), text.col=c("blue", "red"))
```
The 2-batch experiment is able to retrieve more genes as significant than running all in one batch for the same sample size. This is also observed by Johnson et al., but not commented. We claim that the apparent better result for the two-batch experiment is likely a consequence of the use of ComBat. A test of this is to split batch 3 into two imaginary batches with the same design as for batch 1 and 2, and then look at the p-value distribution compared with the real batch 3.

```{r, tidy=FALSE}
Batch45 = Batch3
Batch45_annot = sampleannotation[dimnames(Batch45)[[2]],]
Batch45_annot$Batch="4"
Batch45_annot$Batch[Batch45_annot$Type=="C"][1:4] = "5"
Batch45_annot$Batch[Batch45_annot$Type=="R"][1:3] = "5"
print(table(sampleannotation[sampleannotation$Batch %in% c("1", "2"), c("Batch", "Type")]))
print(table(Batch45_annot[, c("Batch", "Type")]))
```
Then adjust the data with fictive batches and calculate p-values as done for the unadjusted batch 3.
```{r, results='hide', tidy=FALSE}
EB45 = as.matrix( sva::ComBat(
            dat=Batch45, 
            batch=Batch45_annot[colnames(Batch45),"Batch"], 
            mod=model.matrix( ~as.factor(Batch45_annot[colnames(Batch45),"Type"])  ), 
            numCovs=NULL, 
            par.prior=useparprior, 
            prior.plots=FALSE))

EB45_pvals = apply(EB45, 1 , 
                  FUN=function(x){t.test(
                                    x[Batch45_annot[colnames(EB45), "Type"]=="C"],
                                    x[Batch45_annot[colnames(EB45), "Type"]=="R"]
                                    )$p.value})
```
```{r, dev='svg', fig.width=8, fig.height=8, tidy=FALSE}
hist(EB45_pvals, border="blue",  main="P-values, real data", breaks=100, xlab="p-value")
hist(Batch3_pvals, border="red", add=T, breaks=100)
legend("topright", legend=c("ComBat adjusted fictive-batches of batch 3", "Batch 3 not adjusted"), text.col=c("blue", "red"))
```
The fictive-batches version with ComBat seemingly outperforms the original. This plot looks very similar to the EB2 vs. Batch3 p-values, and for both, the p-values for the ComBat adjusted data are likely deflated due to ComBat use.

The fictive batch asssignment above was just one of several possible. Repeating the selection 10 times shows that the lower p-values was not a coincidence.

```{r, tidy=FALSE, results='hide'}
fictivebatches_pvalcounts=list()
runs = 10
for(i in 1:runs)
{
Batch45 = Batch3
Batch45_annot = sampleannotation[dimnames(Batch45)[[2]],]
Batch45_annot$Batch="4"
Batch45_annot$Batch[Batch45_annot$Type=="C"][sample(1:sum(Batch45_annot$Type=="C"), 4)] = "5"
Batch45_annot$Batch[Batch45_annot$Type=="R"][sample(1:sum(Batch45_annot$Type=="R"), 3)] = "5"
print(table(sampleannotation[sampleannotation$Batch %in% c("1", "2"), c("Batch", "Type")]))
print(table(Batch45_annot[, c("Batch", "Type")]))

EB45 = as.matrix( sva::ComBat(
            dat=Batch45, 
            batch=Batch45_annot[colnames(Batch45),"Batch"], 
            mod=model.matrix( ~as.factor(Batch45_annot[colnames(Batch45),"Type"])  ), 
            numCovs=NULL, 
            par.prior=TRUE, 
            prior.plots=FALSE))

pvals = apply(EB45, 1 , 
                  FUN=function(x){t.test(
                                    x[Batch45_annot[colnames(EB45), "Type"]=="C"],
                                    x[Batch45_annot[colnames(EB45), "Type"]=="R"]
                                    )$p.value})

fictivebatches_pvalcounts[[i]] = hist(pvals, plot=FALSE, breaks=100)$counts
}
```
In order to save some computational time the parameteric version of ComBat was used for this.
```{r, dev='svg', fig.width=8, fig.height=8, tidy=FALSE}
plot((1:20)/100, hist(Batch3_pvals, plot=FALSE, breaks=100)$counts[1:20], col="red",  main="P-values, fictive batches", 
      xlab="p-value", ylab="Frequency", type="l", lwd=2, 
      ylim=c(0, max(unlist(fictivebatches_pvalcounts))))
for(i in 1:length(fictivebatches_pvalcounts))
{
  lines((1:20)/100,fictivebatches_pvalcounts[[i]][1:20], col="blue")
}
legend("topright", legend=c("ComBat adjusted fictive-batches of batch 3", "Batch 3 not adjusted"), text.col=c("blue", "red"))
```


```{r}
source("../../commonscripts/helper_functions.r")
nshuffleddatasets = 10
datamatrices_permuted = list()
for(i in 1:nshuffleddatasets)
{
  x = shufflesamplesinbatch( rownames(sampleannotation), sampleannotation$Batch, sampleannotation$Type, c("C", "R") )
  print(  sum(sampleannotation[,"Type"] != sampleannotation[x,"Type"]  ))  # see how many changed
  datamatrices_permuted[[i]] = as.matrix(ComBat(dat=datamatrix[, x], 
                                                batch=sampleannotation$Batch, 
                                                mod=model.matrix(~as.factor(sampleannotation[colnames(datamatrix),"Type"])),  
                                                numCovs=NULL, 
                                                par.prior=TRUE, 
                                                prior.plots=FALSE))
}

permuted_pvalcounts=list()
for(i in 1:nshuffleddatasets)
{ 
  this_p = apply(datamatrices_permuted[[i]], 1 , 
                  FUN=function(x){t.test(
                                    x[sampleannotation[, "Type"]=="C"],
                                    x[sampleannotation[, "Type"]=="R"]
                                    )$p.value})

  permuted_pvalcounts[[i]] = hist(this_p, plot=FALSE, breaks=100)$counts
  print(table(p.adjust(this_p, method="fdr")<0.05))
}
```


Forslag figur brukt i artikkel.
```{r pvaluesjohnson, dev='svg', fig.width=10, fig.height=10, tidy=FALSE}
# figure used in our article
# NB! bruker orgniale data som ikke er floored eller log2!
par(mfrow=c(2, 2))

# reproduced ComBat + limma
hist(EB3_limma_res$p.value[,1], main="P-values, real data, ComBat followed by Limma", breaks=100, xlab="p-value", ylim=c(0,3000))

# batch handled in limma
hist(datamatrix_limma_res$p.value[,1], breaks=100, main="P-values, real data, batch handled by Limma", xlab="p-value" , ylim=c(0,3000))

# random ComBat + limma
hist(EBrand_limma_pvalues, main="P-values, Random data, ComBat followed by Limma", breaks=100, xlab="p-value" , ylim=c(0,3000))

# permuted labels whitin batch

### permutation sanity test
real_pvalcounts=hist(EB3_limma_res$p.value[,1], plot=FALSE, breaks=100)$counts
plot((1:20)/100, real_pvalcounts[1:20], col="red",
     main=paste("P-values, real data and ", length(permuted_pvalcounts), " permutations" ,sep=""),
      xlab="p-value", ylab="Frequency", type="l", lwd=2, 
      ylim=c(0, max(unlist( c(permuted_pvalcounts,real_pvalcounts)))))
for(i in 1:length(permuted_pvalcounts))
{
  lines((1:20)/100,permuted_pvalcounts[[i]][1:20], col="blue")
}
legend("topright", legend=c("Reproduced p-values real labels", "Permuted C and R labels"),
       text.col=c("red", "blue"))
```


Forslag 2 figur brukt i artikkel.
```{r pvaluesjohnson2, dev='svg', fig.width=10, fig.height=10, tidy=FALSE}
# figure used in our article
# NB! bruker orgniale data som ikke er floored eller log2!
par(mfrow=c(1, 1))

# reproduced ComBat + limma
hist(EB3_limma_res$p.value[,1], main="P-values, 3 settings", breaks=100, xlab="p-value", ylim=c(0,3000), border="red")

# batch handled in limma
hist(datamatrix_limma_res$p.value[,1], add=T, breaks=100, xlab="p-value" , ylim=c(0,3000), border="blue")

# random ComBat + limma
hist(EBrand_limma_pvalues, add=T, breaks=100, xlab="p-value" , ylim=c(0,3000), border="black")

legend("topright", legend=c("Reald data ComBat adjusted", "Reald data Limma adjusted", "Random data ComBat adjusted"),
       text.col=c("red", "blue", "black"))
```


### References


Johnson, WE, Rabinovic, A, and Li, C (2007). Adjusting batch effects in microarray expression data using Empirical Bayes methods. Biostatistics 8(1):118-127.

Storey, J. D. and Tibshirani, R. (2003) Proc Natl Acad Sci U S A, 100, 9440-5.

Raivo Kolde (2013). pheatmap: Pretty Heatmaps. R package version 0.7.7. http://CRAN.R-project.org/package=pheatmap

Leek JT, Johnson WE, Parker HS, Jaffe AE, Storey JD.(2012) The sva package for removing batch effects and other unwanted variation in high-throughput experiments. Bioinformatics. 2012 Mar 15;28(6):882-3.

Smyth, GK (2005). Limma: linear models for microarray data. In: 'Bioinformatics and Computational Biology Solutions
  using R and Bioconductor'. R. Gentleman, V. Carey, S. Dudoit, R. Irizarry, W. Huber (eds), Springer, New York, pages
  397-420.
  
  R Core Team (2013). R: A language and environment for statistical computing. R Foundation for Statistical Computing,
  Vienna, Austria. URL http://www.R-project.org/

  Yihui Xie (2013). knitr: A general-purpose package for dynamic report generation in R. R package version 1.5.

  Yihui Xie (2013) Dynamic Documents with R and knitr. Chapman and Hall/CRC. ISBN 978-1482203530

  Yihui Xie (2013) knitr: A Comprehensive Tool for Reproducible Research in R. In Victoria Stodden, Friedrich Leisch and
  Roger D. Peng, editors, Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595
  
  RStudio Team (2012). RStudio: Integrated Development for R. RStudio, Inc., Boston, MA URL http://www.rstudio.com/.


```{r sessionInfo, comment=""}
sessionInfo()
```

generation ended `r as.character(Sys.time())`. Time spent `r  as.integer(round(difftime(Sys.time(),starttime, units="mins")))` minutes .

